## Legacy Deeplearning 네트워크, Transformer 이전 모델들

딥러닝의 역사는 인간 두뇌를 모방하려는 시도에서 시작되었으며, Transformer 아키텍처가 등장하기 전까지 다양한 신경망 모델들이 개발되었습니다. 이러한 모델들은 각각 고유한 특성과 한계를 가지고 있었습니다.

## 초기 신경망 모델

### 퍼셉트론(Perceptron)

1950년대에 개발된 가장 초기의 인공 신경망 모델로, 단일 층 구조를 가지고 있었습니다. 

1969년 Marvin Minsky와 Seymour Papert는 "Perceptrons"라는 책을 출판하여 단순 신경망의 한계를 설명했고, 이로 인해 신경망 연구가 감소하고 기호적 AI 연구가 번창하게 되었습니다.

### 다층 퍼셉트론(MLP)

여러 층의 뉴런으로 구성된 피드포워드 신경망으로, 1969년 Arthur Bryson과 Yu-Chi Ho가 역전파 학습 알고리즘을 설명하면서 발전했습니다. 

이는 퍼셉트론의 발전된 형태이자 딥러닝의 기초가 되었습니다.

## 컨볼루션 신경망(CNN)

1979년 Kunihiko Fukushima가 네오코그니트론(neocognitron)을 발표했으며, 이는 패턴 인식 작업에 사용되는 계층적, 다층 인공 신경망이었습니다. 

1989년 Yann LeCun, Yoshua Bengio, Patrick Haffner는 CNN이 손글씨 문자 인식에 사용될 수 있음을 보여주었고, 신경망이 실제 문제에 적용될 수 있음을 증명했습니다.

CNN은 주로 컴퓨터 비전 분야에서 사용되었으며, Transformer 이전 시대에 이미지 처리의 주요 아키텍처였습니다.

## 순환 신경망(RNN)

### 기본 RNN

시퀀스 데이터(텍스트, 시계열 등)를 처리하기 위해 개발된 RNN은 내부 메모리를 유지하여 이전 입력이 미래 출력에 영향을 미치도록 설계되었습니다. 

그러나 긴 시퀀스에서는 초기 입력의 영향이 감소하는 '기울기 소실 문제'로 인해 장기 의존성을 처리하는 데 어려움이 있었습니다.

### LSTM(Long Short-Term Memory)

1995년에 개발된 LSTM은 RNN의 한계를 극복하기 위한 중요한 혁신이었습니다. 

LSTM은 다양한 혁신을 통해 기울기 소실 문제를 해결하여 긴 시퀀스 모델링의 효율적인 학습을 가능하게 했습니다.

게이트 메커니즘을 사용하여 정보 흐름을 조절하고, 중요한 정보를 긴 시퀀스에서도 유지할 수 있게 해 언어 모델링과 텍스트 생성 같은 작업에 더 효과적이었습니다.

### GRU(Gated Recurrent Unit)

LSTM의 변형으로, 더 단순한 구조를 가지면서도 유사한 성능을 제공했습니다.

GRU는 LSTM보다 계산 비용이 적어 효율적인 대안으로 사용되었습니다.

## 오토인코더(Autoencoder)

### 기본 오토인코더

손상되거나 흐릿한 이미지를 재구성하는 등 다양한 목적으로 사용되었습니다. 

입력 데이터를 압축하고 다시 원래 형태로 복원하는 과정을 학습합니다.

### 변분 오토인코더(VAE)

2014년 Diederik Kingma와 Max Welling이 소개한 VAE는 데이터를 재구성할 뿐만 아니라 원본 데이터의 변형을 출력하는 중요한 능력을 추가했습니다. 

이 새로운 데이터 생성 능력은 GAN(Generative Adversarial Network)에서 확산 모델에 이르기까지 점점 더 현실적인 가짜 이미지를 생성할 수 있는 새로운 기술의 빠른 발전을 촉발했습니다.

## 인코더-디코더 아키텍처

Transformer 이전에 시퀀스-투-시퀀스 작업을 위해 개발된 이 아키텍처는 주로 기계 번역에 사용되었습니다. 

인코더는 입력 시퀀스를 처리하여 컨텍스트 벡터를 생성하고, 디코더는 이 벡터를 사용하여 출력 시퀀스를 생성합니다.

## Transformer로의 전환

2017년 "Attention Is All You Need" 논문에서 소개된 Transformer는 시퀀셜 데이터 처리 방식을 혁신했습니다. 

RNN과 LSTM이 데이터를 순차적으로 처리하는 반면, Transformer는 전체 시퀀스를 동시에 처리하여 병렬 처리가 가능하고 데이터 내 복잡한 관계를 포착하는 데 더 뛰어났습니다.

이러한 Legacy 모델들은 각각의 한계에도 불구하고 현대 딥러닝의 기반을 마련했으며, Transformer의 등장으로 AI 방법론이 통합되는 추세를 보이게 되었습니다. 

언어, 비전, 로봇공학, 생물학 등 다양한 영역에서 Transformer가 최첨단 성능을 입증하면서 "모든 것을 지배하는 하나의 AI 아키텍처"로 향하는 경향이 나타났습니다.

